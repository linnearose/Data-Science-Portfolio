{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/animal_data')  # r/aww  r/cats  r/dogs\n",
    "# df = pd.read_csv('./data/all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "subreddit       0\n",
       "name            0\n",
       "is_video        0\n",
       "id              0\n",
       "duplicate       0\n",
       "clean_title    34\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>name</th>\n",
       "      <th>is_video</th>\n",
       "      <th>id</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>üòçüòç</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a7x55r</td>\n",
       "      <td>False</td>\n",
       "      <td>a7x55r</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>:)</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a7ydcz</td>\n",
       "      <td>False</td>\n",
       "      <td>a7ydcz</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>- who is that there?</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a80i4e</td>\n",
       "      <td>False</td>\n",
       "      <td>a80i4e</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>:3</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a7yc7f</td>\n",
       "      <td>False</td>\n",
       "      <td>a7yc7f</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>.</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a7y3dc</td>\n",
       "      <td>False</td>\n",
       "      <td>a7y3dc</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>:) :) :) :)</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a84v4l</td>\n",
       "      <td>False</td>\n",
       "      <td>a84v4l</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Just this...</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a899ci</td>\n",
       "      <td>False</td>\n",
       "      <td>a899ci</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>:3</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a8anr2</td>\n",
       "      <td>False</td>\n",
       "      <td>a8anr2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>üê∞</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a880w1</td>\n",
       "      <td>False</td>\n",
       "      <td>a880w1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>üòç</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a87f77</td>\n",
       "      <td>False</td>\n",
       "      <td>a87f77</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>what are you doing....??</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a8a10o</td>\n",
       "      <td>False</td>\n",
       "      <td>a8a10o</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Is this how you do it?</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a9zw72</td>\n",
       "      <td>False</td>\n",
       "      <td>a9zw72</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>üêïüêï</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a9w7ia</td>\n",
       "      <td>False</td>\n",
       "      <td>a9w7ia</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>what</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_aa1z8k</td>\n",
       "      <td>False</td>\n",
       "      <td>aa1z8k</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>üòâ</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_a9t3xv</td>\n",
       "      <td>False</td>\n",
       "      <td>a9t3xv</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>Where to?</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_aa37n0</td>\n",
       "      <td>False</td>\n",
       "      <td>aa37n0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>They are</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_aaa6lq</td>\n",
       "      <td>False</td>\n",
       "      <td>aaa6lq</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>‚ù£Ô∏è</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abjw7e</td>\n",
       "      <td>False</td>\n",
       "      <td>abjw7e</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>:}</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abjufu</td>\n",
       "      <td>False</td>\n",
       "      <td>abjufu</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>‚ù§Ô∏è</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abkp79</td>\n",
       "      <td>False</td>\n",
       "      <td>abkp79</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>üêà</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abisiz</td>\n",
       "      <td>False</td>\n",
       "      <td>abisiz</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>üêàüêà</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abkz3n</td>\n",
       "      <td>False</td>\n",
       "      <td>abkz3n</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>üî•üêæüê∂</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abl6rk</td>\n",
       "      <td>False</td>\n",
       "      <td>abl6rk</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>What was that?!</td>\n",
       "      <td>aww</td>\n",
       "      <td>t3_abies6</td>\n",
       "      <td>False</td>\n",
       "      <td>abies6</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>She's out</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_a8a3s3</td>\n",
       "      <td>False</td>\n",
       "      <td>a8a3s3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>1M</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_a881wb</td>\n",
       "      <td>False</td>\n",
       "      <td>a881wb</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>üç¨</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_a9p6wg</td>\n",
       "      <td>False</td>\n",
       "      <td>a9p6wg</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>O.O</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_a9kgm9</td>\n",
       "      <td>False</td>\n",
       "      <td>a9kgm9</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6172</th>\n",
       "      <td>ü•Ç</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_a9ka3v</td>\n",
       "      <td>False</td>\n",
       "      <td>a9ka3v</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>What?</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_aaem2k</td>\n",
       "      <td>False</td>\n",
       "      <td>aaem2k</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>üí§üí§</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_aacnp6</td>\n",
       "      <td>True</td>\n",
       "      <td>aacnp6</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>No!</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_aabo20</td>\n",
       "      <td>True</td>\n",
       "      <td>aabo20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>What is he doing?</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_abclps</td>\n",
       "      <td>False</td>\n",
       "      <td>abclps</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td>Where are you?</td>\n",
       "      <td>cats</td>\n",
       "      <td>t3_abae0w</td>\n",
       "      <td>False</td>\n",
       "      <td>abae0w</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title subreddit       name  is_video      id  \\\n",
       "125                         üòçüòç       aww  t3_a7x55r     False  a7x55r   \n",
       "203                         :)       aww  t3_a7ydcz     False  a7ydcz   \n",
       "711       - who is that there?       aww  t3_a80i4e     False  a80i4e   \n",
       "748                         :3       aww  t3_a7yc7f     False  a7yc7f   \n",
       "822                          .       aww  t3_a7y3dc     False  a7y3dc   \n",
       "988                :) :) :) :)       aww  t3_a84v4l     False  a84v4l   \n",
       "989               Just this...       aww  t3_a899ci     False  a899ci   \n",
       "1030                        :3       aww  t3_a8anr2     False  a8anr2   \n",
       "1175                         üê∞       aww  t3_a880w1     False  a880w1   \n",
       "1193                         üòç       aww  t3_a87f77     False  a87f77   \n",
       "1271  what are you doing....??       aww  t3_a8a10o     False  a8a10o   \n",
       "1801    Is this how you do it?       aww  t3_a9zw72     False  a9zw72   \n",
       "1802                        üêïüêï       aww  t3_a9w7ia     False  a9w7ia   \n",
       "2034                      what       aww  t3_aa1z8k     False  aa1z8k   \n",
       "2355                         üòâ       aww  t3_a9t3xv     False  a9t3xv   \n",
       "2377                 Where to?       aww  t3_aa37n0     False  aa37n0   \n",
       "2683                  They are       aww  t3_aaa6lq     False  aaa6lq   \n",
       "3656                        ‚ù£Ô∏è       aww  t3_abjw7e     False  abjw7e   \n",
       "3739                        :}       aww  t3_abjufu     False  abjufu   \n",
       "3982                        ‚ù§Ô∏è       aww  t3_abkp79     False  abkp79   \n",
       "4134                         üêà       aww  t3_abisiz     False  abisiz   \n",
       "4136                        üêàüêà       aww  t3_abkz3n     False  abkz3n   \n",
       "4157                       üî•üêæüê∂       aww  t3_abl6rk     False  abl6rk   \n",
       "4222           What was that?!       aww  t3_abies6     False  abies6   \n",
       "4602                 She's out      cats  t3_a8a3s3     False  a8a3s3   \n",
       "4774                        1M      cats  t3_a881wb     False  a881wb   \n",
       "5907                         üç¨      cats  t3_a9p6wg     False  a9p6wg   \n",
       "6036                       O.O      cats  t3_a9kgm9     False  a9kgm9   \n",
       "6172                         ü•Ç      cats  t3_a9ka3v     False  a9ka3v   \n",
       "6323                     What?      cats  t3_aaem2k     False  aaem2k   \n",
       "6496                        üí§üí§      cats  t3_aacnp6      True  aacnp6   \n",
       "6540                       No!      cats  t3_aabo20      True  aabo20   \n",
       "7347         What is he doing?      cats  t3_abclps     False  abclps   \n",
       "7364            Where are you?      cats  t3_abae0w     False  abae0w   \n",
       "\n",
       "      duplicate clean_title  \n",
       "125       False         NaN  \n",
       "203       False         NaN  \n",
       "711       False         NaN  \n",
       "748       False         NaN  \n",
       "822       False         NaN  \n",
       "988       False         NaN  \n",
       "989       False         NaN  \n",
       "1030      False         NaN  \n",
       "1175      False         NaN  \n",
       "1193      False         NaN  \n",
       "1271      False         NaN  \n",
       "1801      False         NaN  \n",
       "1802      False         NaN  \n",
       "2034      False         NaN  \n",
       "2355      False         NaN  \n",
       "2377      False         NaN  \n",
       "2683      False         NaN  \n",
       "3656      False         NaN  \n",
       "3739      False         NaN  \n",
       "3982      False         NaN  \n",
       "4134      False         NaN  \n",
       "4136      False         NaN  \n",
       "4157      False         NaN  \n",
       "4222      False         NaN  \n",
       "4602      False         NaN  \n",
       "4774      False         NaN  \n",
       "5907      False         NaN  \n",
       "6036      False         NaN  \n",
       "6172      False         NaN  \n",
       "6323      False         NaN  \n",
       "6496      False         NaN  \n",
       "6540      False         NaN  \n",
       "7347      False         NaN  \n",
       "7364      False         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all rows where clean title was null\n",
    "df[df['clean_title'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null rows\n",
    "df.dropna(inplace=True, \n",
    "          axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "subreddit      0\n",
       "name           0\n",
       "is_video       0\n",
       "id             0\n",
       "duplicate      0\n",
       "clean_title    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check all null rows are gone\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and target variable\n",
    "X = df['clean_title']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would cuter hiding u medicine'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6317                   like whiskey barrel thanks hoomans\n",
       "4617    little princess prefers organic food handmade ...\n",
       "3590                                     freedom good boy\n",
       "2987                             merry christmas gato cat\n",
       "4195                     chewie contemplating life choice\n",
       "Name: clean_title, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None,\n",
    "                            stop_words=None,\n",
    "                            max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = pd.DataFrame(X_train_vect.toarray(), \n",
    "                            columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "tree = DecisionTreeClassifier(criterion='gini',\n",
    "                              min_samples_split=4, # doesn't change after 4\n",
    "                              min_samples_leaf=5, # doesn't change after 5\n",
    "                              max_depth=5) # 5 and 6 yield best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "tree.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774748923959828"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6673838209982789"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "lr.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8559540889526542"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_vect, y_train)  # Overfit - one reason why RandomTrees is a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6897590361445783"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Logistic Regression (LR) and Random Trees (RT) had close to the same scores when differentiating between 2 subreddits. However, RT performed significantly better than LR when differentiating between 3 or more subreddits. \n",
    "\n",
    "To evaluate how effective a model is, you can look at the score value itself, but also the difference between the `train` score and the `test` score. Here, we can see that RT is a more effective model because the train and test scores match up well, which means the model isn't too overfit or underfit.\n",
    "\n",
    "If you look at LR in both cases below, the differences between the `train` and `test` score are much higher than those of the RT model. This means that an LR model in this case is *overfitting* the data.\n",
    "\n",
    "Also note that the higher scores for LR shouldn't trick you into thinking it's the better model; what's more important here is the **difference** between the `train` and `test` scores.\n",
    "\n",
    "##### `cats` vs. `dogs` \n",
    "\n",
    "|        | `train` score | `test` score |  difference |\n",
    "|------  |------         |------        |------  |        \n",
    "| **LR** |   0.994 |   0.977  | *0.017* |\n",
    "| **RT** |   0.977 |   0.972  | *0.005* |\n",
    "\n",
    "\n",
    "##### `cats` vs. `dogs` vs. `aww`\n",
    "\n",
    "|        | `train` score | `test` score |  difference |\n",
    "|------  |------         |------        |------  |\n",
    "| **LR** |   0.879 |   0.716  | *0.163* |\n",
    "| **RT** |   0.689 |   0.687  | *0.002* |\n",
    "\n",
    "\n",
    "##### `cats` vs. `dogs` vs. `aww` vs. `datascience`\n",
    "\n",
    "|        | `train` score | `test` score |  difference |\n",
    "|------  |------         |------        |------  |\n",
    "| **LR** |   0.875 |   0.723  | *0.152* |\n",
    "| **RT** |   0.648 |   0.647  | *0.001* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "a = tree.predict(X_test_vect)\n",
    "b = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn numpy.ndarray into Series\n",
    "a = pd.Series(data=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "b = b.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate true subreddit column & prediction column\n",
    "both = pd.concat([a,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 0 column\n",
    "both.rename(columns={0:'original'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All mismatches\n",
    "# both[both['original'] != both['subreddit']]\n",
    "len(both[both['original'] != both['subreddit']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aww --> cats\n",
    "aww = both[both['original'] == 'aww']\n",
    "aww_cats = aww[aww['subreddit'] == 'cats']\n",
    "\n",
    "len(aww_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aww --> dogs\n",
    "aww = both[both['original'] == 'aww']\n",
    "aww_dogs = aww[aww['subreddit'] == 'dogs']\n",
    "\n",
    "len(aww_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cats --> aww\n",
    "aww = both[both['original'] == 'cats']\n",
    "aww_cats = aww[aww['subreddit'] == 'aww']\n",
    "\n",
    "len(aww_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dogs --> aww\n",
    "aww = both[both['original'] == 'dogs']\n",
    "aww_dogs = aww[aww['subreddit'] == 'aww']\n",
    "\n",
    "len(aww_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aww --> datascience\n",
    "aww = both[both['original'] == 'aww']\n",
    "aww_ds = aww[aww['subreddit'] == 'datascience']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datascience --> ~datascience\n",
    "ds = both[both['original'] == 'datascience']\n",
    "aww_ds = ds[ds['subreddit'] == 'dogs']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dogs --> datascience\n",
    "aww = both[both['original'] == 'dogs']\n",
    "aww_ds = aww[aww['subreddit'] == 'datascience']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cats --> datascience\n",
    "aww = both[both['original'] == 'cats']\n",
    "aww_ds = aww[aww['subreddit'] == 'datascience']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cats --> dogs\n",
    "aww = both[both['original'] == 'cats']\n",
    "aww_ds = aww[aww['subreddit'] == 'dogs']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dogs --> cats\n",
    "aww = both[both['original'] == 'dogs']\n",
    "aww_ds = aww[aww['subreddit'] == 'cats']\n",
    "\n",
    "len(aww_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Since `aww` could contain **anything cute and/or fluffy**, it makes sense that `aww` posts are being predicted incorrectly as `cats` or `dogs` relatively frequently - they might be posts about a cat or a dog that happened to be posted in the `aww` subreddit.\n",
    "\n",
    "However, in the other direction, there aren't nearly as many `cats` or `dogs` posts being predicted incorrectly as `aww`. My educated guess here is that there are fewer subject-specific words that end up being strong predictors of *s/aww* since it is a more generalized page. \n",
    "\n",
    "Again, `cats` and `dogs` are a more narrow \"niche\", so the correlation between high-frequency prediction words and each subreddit seems stronger.\n",
    "\n",
    "\n",
    "| original --> prediction | *# of mismatches* | *total mismatches* | *% total mismatches* |\n",
    "|------  |------ |------ |------ |   \n",
    "| **`aww` --> `cats`** | 455 | 729 | 62.4% |\n",
    "| **`aww` --> `dogs`** | 34 | 729 | 4.7% |\n",
    "| **`cats` --> `aww`** | 71 | 729 | 9.7% |\n",
    "| **`dogs` --> `aww`** | 6 | 729 | 0.8% |\n",
    "| **`aww` --> `datascience`** | 149 | 729 | 20.4% |\n",
    "| **`dogs` --> `datascience`** | 8 | 729 | 1.1% |\n",
    "| **`dogs` --> `cats`** | 6 | 729 | 0.8% |\n",
    "\n",
    "There were also 149 `aww` posts that were misidentified as `datascience` posts. Again, I believe that because *s/datascience* is the most niche and most likely has the most unique description words in post titles, more general posts can be miscategorized if they happen to have a key word. \n",
    "\n",
    "No `datascience` posts were mistaken for other subreddits in the predictions - i.e. there were **no false negatives** in this specific case. This continues to prove that the language in *r/datascience* is specific enough to be a strong predictor of that subbreddit category.\n",
    "\n",
    "**After doing some more EDA** and going over this analysis, I wanted to see if there were any other independent variables that would help to distinguish more effectively between `aww` and `cats`. Both of these subreddits have a significant number of posts that include videos - `aww` especially - so this could have an effect on the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
